{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For T5 Models\n",
    "\n",
    "# %pip install sentencepiece\n",
    "\n",
    "# from transformers import RobertaTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Load T5\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# # Load Flan-T5\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# # Load CodeT5\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "# model_inputs = tokenizer(\n",
    "#     [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n",
    "# )\n",
    "# generated_ids = model.generate(**model_inputs)\n",
    "# tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "# ['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n",
    "# 'Portugal is a country in southwestern Europe, on the Iber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers \"bitsandbytes>=0.39.0\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gradio_client import Client\n",
    "\n",
    "# client = Client(\"huggingface-projects/llama-2-13b-chat\")\n",
    "# result = client.predict(\n",
    "# \t\tmessage=prompt,\n",
    "# \t\tsystem_prompt=\"Hello!!\",\n",
    "# \t\tmax_new_tokens=1024,\n",
    "# \t\ttemperature=0.6,\n",
    "# \t\ttop_p=0.9,\n",
    "# \t\ttop_k=50,\n",
    "# \t\trepetition_penalty=1.2,\n",
    "# \t\tapi_name=\"/chat\"\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load JSON data\n",
    "# with open('sample_problem.json', 'r') as file:\n",
    "#     problem_data = json.load(file)\n",
    "\n",
    "# # Sample user submission\n",
    "# user_submission = '''\n",
    "#     Create a count for each character in the string.\n",
    "#     For each character:\n",
    "#     Convert it to lowercase.\n",
    "#     Count how many times it appears.\n",
    "#     Set total_length to 0 and odd_count to 0.\n",
    "#     For each character count:\n",
    "#     Add the even part of the count to total_length.\n",
    "#     If the count is odd, add 1 to odd_count.\n",
    "#     If odd_count is greater than 0, add 1 to total_length.\n",
    "#     Return total_length.\n",
    "# '''\n",
    "\n",
    "# # Define the prompt\n",
    "# # Define the feedback template as a regular string\n",
    "# feedback_template = '''\n",
    "# {\n",
    "#     \"title\": \"Analysis of User Submission\",\n",
    "#     \"analysis\": \"Detailed analysis of the user's approach.\",\n",
    "#     \"summary\": \"Concise summary of the user's approach and correctness.\",\n",
    "#     \"score\": \"An integer score: 1 (incorrect), 2 (partially correct), 3 (correct).\"\n",
    "# }\n",
    "# '''\n",
    "\n",
    "# # Define the prompt\n",
    "# prompt = f'''\n",
    "# You are a professor who specializes in solving code problems.\n",
    "# The user will submit, in words, their approach to solving the problem.\n",
    "# Analyze the user's submission and provide feedback.\n",
    "\n",
    "# Output your feedback in JSON format with the following structure:\n",
    "# {feedback_template}\n",
    "\n",
    "# Here are the problem details:\n",
    "\n",
    "# Category: {problem_data['category']}\n",
    "# Subcategory: {problem_data['subcategory']}\n",
    "# Difficulty: {problem_data['difficulty']}\n",
    "# Title: {problem_data['title']}\n",
    "# Description: {problem_data['description']}\n",
    "# Constraints: \n",
    "# {chr(10).join([f'{constraint}' for constraint in problem_data['constraints']])}\n",
    "\n",
    "# Examples:\n",
    "# {chr(10).join([f'Input: {example[\"input\"]}, Output: {example[\"output\"]}, Explanation: {example[\"explanation\"]}' for example in problem_data['examples']])}\n",
    "\n",
    "# Here is the user's submission:\n",
    "# {user_submission}\n",
    "\n",
    "# Ensure your response is coherent and follows the JSON structure provided.\n",
    "# '''\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "# # prompt = \"Can you summarize the capabilities of language models, including tasks such as summarization, question answering, and text generation?\"\n",
    "\n",
    "# # Set pad_token to eos_token\n",
    "# # tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # # Tokenize the prompt and set attention mask\n",
    "# # encoded_input = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample user submission\n",
    "# user_submission = '''\n",
    "#     Create a count for each character in the string.\n",
    "#     For each character:\n",
    "#     Convert it to lowercase.\n",
    "#     Count how many times it appears.\n",
    "#     Set total_length to 0 and odd_count to 0.\n",
    "#     For each character count:\n",
    "#     Add the even part of the count to total_length.\n",
    "#     If the count is odd, add 1 to odd_count.\n",
    "#     If odd_count is greater than 0, add 1 to total_length.\n",
    "#     Return total_length.\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the prompt\n",
    "# # Define the feedback template as a regular string\n",
    "# feedback_template = '''\n",
    "# {\n",
    "#     \"title\": \"Analysis of User Submission\",\n",
    "#     \"analysis\": \"Detailed analysis of the user's approach.\",\n",
    "#     \"summary\": \"Concise summary of the user's approach and correctness.\",\n",
    "#     \"score\": \"An integer score: 1 (incorrect), 2 (partially correct), 3 (correct).\"\n",
    "# }\n",
    "# '''\n",
    "\n",
    "# # Define the prompt\n",
    "# prompt = f'''\n",
    "# You are a professor who specializes in solving code problems.\n",
    "# The user will submit, in words, their approach to solving the problem.\n",
    "# Analyze the user's submission and provide feedback.\n",
    "\n",
    "# Output your feedback in JSON format with the following structure:\n",
    "# {feedback_template}\n",
    "\n",
    "# Here are the problem details:\n",
    "\n",
    "# Category: {problem_data['category']}\n",
    "# Subcategory: {problem_data['subcategory']}\n",
    "# Difficulty: {problem_data['difficulty']}\n",
    "# Title: {problem_data['title']}\n",
    "# Description: {problem_data['description']}\n",
    "# Constraints: \n",
    "# {chr(10).join([f'{constraint}' for constraint in problem_data['constraints']])}\n",
    "\n",
    "# Examples:\n",
    "# {chr(10).join([f'Input: {example[\"input\"]}, Output: {example[\"output\"]}, Explanation: {example[\"explanation\"]}' for example in problem_data['examples']])}\n",
    "\n",
    "# Here is the user's submission:\n",
    "# {user_submission}\n",
    "\n",
    "# Ensure your response is coherent and follows the JSON structure provided.\n",
    "# '''\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "# # prompt = \"Can you summarize the capabilities of language models, including tasks such as summarization, question answering, and text generation?\"\n",
    "\n",
    "# # Set pad_token to eos_token\n",
    "# # tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # # Tokenize the prompt and set attention mask\n",
    "# # encoded_input = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate feedback with attention mask\n",
    "# output_ids = model.generate(\n",
    "#     input_ids=encoded_input['input_ids'],\n",
    "#     attention_mask=encoded_input['attention_mask'],\n",
    "#     max_length=250,\n",
    "#     max_new_tokens=50,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.5,\n",
    "#     top_k=50,\n",
    "#     top_p=0.95,\n",
    "#     repetition_penalty = 5.0\n",
    "# )\n",
    "\n",
    "# # Decode the output\n",
    "# feedback_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# print(feedback_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "# model_inputs = tokenizer(\n",
    "#     [prompt], return_tensors=\"pt\", padding=True\n",
    "# )\n",
    "# generated_ids = model.generate(**model_inputs, max_new_tokens=500)\n",
    "# tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# import torch\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0,load_in_4bit=True\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to('cpu')\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,device='cpu')\n",
    "# prompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:\"\n",
    "\n",
    "# result = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\n",
    "# model = model.to('cpu')\n",
    "# # infer\n",
    "# prompt = \"<human>: Who is Alan Turing?\\n<bot>:\"\n",
    "# inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "# input_length = inputs.input_ids.shape[1]\n",
    "# outputs = model.generate(\n",
    "#     **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
    "# )\n",
    "# token = outputs.sequences[0, input_length:]\n",
    "# output_str = tokenizer.decode(token)\n",
    "# print(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_codescript",
   "language": "python",
   "name": "venv_codescript"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
