{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For T5 Models\n",
    "\n",
    "# %pip install sentencepiece\n",
    "\n",
    "from transformers import RobertaTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Load T5\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# # Load Flan-T5\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# # Load CodeT5\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> red, blue <pad> <pad> <pad> <pad> <pad> <pad> <pad>',\n",
       " '<pad> Portugal is the second largest country in the world.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n",
    ")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "# ['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n",
    "# 'Portugal is a country in southwestern Europe, on the Iber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers \"bitsandbytes>=0.39.0\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://huggingface-projects-llama-2-13b-chat.hf.space âœ”\n",
      " {\n",
      "\"title\": \"Analysis of User Submission\",\n",
      "\"analysis\": \"The user has proposed an algorithm to solve the Longest Palindrome problem. However, there are some issues with the approach that need to be addressed.\",\n",
      "\"summary\": \"The user has suggested counting the frequency of each character in the input string and using that information to determine the longest palindrome. However, this approach does not take into account the fact that characters may appear multiple times in the string, leading to incorrect results.\",\n",
      "\"score\": \"1 (incorrect)\"\n",
      "}\n",
      "\n",
      "The user's submission contains several steps:\n",
      "\n",
      "1. Creating a count for each character in the string.\n",
      "2. Converting each character to lowercase.\n",
      "3. Counting how many times each character appears.\n",
      "4. Setting total_length to 0 and odd_count to 0.\n",
      "5. Adding the even part of the count to total_length.\n",
      "6. Checking if the count is odd and adding 1 to odd_count if necessary.\n",
      "7. Returning total_length.\n",
      "\n",
      "However, there are several issues with this approach:\n",
      "\n",
      "1. It does not handle cases where characters appear more than once in the string. This can lead to incorrect results.\n",
      "2. It does not consider the fact that some characters may not appear at all in the string, leading to incorrect results.\n",
      "3. The algorithm does not check for palindromes correctly, as it only considers the individual counts of each character without considering the overall pattern of the string.\n",
      "\n",
      "To improve this solution, the user should focus on finding a more efficient way to identify and count palindromic sequences within the input string. Additionally, they should ensure that their solution handles all possible input scenarios, including strings with repeated characters and strings with no repeating characters.\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"huggingface-projects/llama-2-13b-chat\")\n",
    "result = client.predict(\n",
    "\t\tmessage=prompt,\n",
    "\t\tsystem_prompt=\"Hello!!\",\n",
    "\t\tmax_new_tokens=1024,\n",
    "\t\ttemperature=0.6,\n",
    "\t\ttop_p=0.9,\n",
    "\t\ttop_k=50,\n",
    "\t\trepetition_penalty=1.2,\n",
    "\t\tapi_name=\"/chat\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data\n",
    "with open('sample_problem.json', 'r') as file:\n",
    "    problem_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample user submission\n",
    "user_submission = '''\n",
    "    Create a count for each character in the string.\n",
    "    For each character:\n",
    "    Convert it to lowercase.\n",
    "    Count how many times it appears.\n",
    "    Set total_length to 0 and odd_count to 0.\n",
    "    For each character count:\n",
    "    Add the even part of the count to total_length.\n",
    "    If the count is odd, add 1 to odd_count.\n",
    "    If odd_count is greater than 0, add 1 to total_length.\n",
    "    Return total_length.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a professor who specializes in solving code problems.\n",
      "The user will submit, in words, their approach to solving the problem.\n",
      "Analyze the user's submission and provide feedback.\n",
      "\n",
      "Output your feedback in JSON format with the following structure:\n",
      "\n",
      "{\n",
      "    \"title\": \"Analysis of User Submission\",\n",
      "    \"analysis\": \"Detailed analysis of the user's approach.\",\n",
      "    \"summary\": \"Concise summary of the user's approach and correctness.\",\n",
      "    \"score\": \"An integer score: 1 (incorrect), 2 (partially correct), 3 (correct).\"\n",
      "}\n",
      "\n",
      "\n",
      "Here are the problem details:\n",
      "\n",
      "Category: manipulations\n",
      "Subcategory: strings\n",
      "Difficulty: easy\n",
      "Title: Longest Palindrome\n",
      "Description: Given a string s which consists of lowercase or uppercase letters, return the length of the longest palindrome that can be built with those letters. Letters are case sensitive, for example, 'Aa' is not considered a palindrome.\n",
      "Constraints: \n",
      "1 <= s.length <= 2000\n",
      "s consists of lowercase and/or uppercase English letters only.\n",
      "\n",
      "Examples:\n",
      "Input: abccccdd, Output: 7, Explanation: One longest palindrome that can be built is 'dccaccd', whose length is 7.\n",
      "Input: a, Output: 1, Explanation: The longest palindrome that can be built is 'a', whose length is 1.\n",
      "\n",
      "Here is the user's submission:\n",
      "\n",
      "    Create a count for each character in the string.\n",
      "    For each character:\n",
      "    Convert it to lowercase.\n",
      "    Count how many times it appears.\n",
      "    Set total_length to 0 and odd_count to 0.\n",
      "    For each character count:\n",
      "    Add the even part of the count to total_length.\n",
      "    If the count is odd, add 1 to odd_count.\n",
      "    If odd_count is greater than 0, add 1 to total_length.\n",
      "    Return total_length.\n",
      "\n",
      "\n",
      "Ensure your response is coherent and follows the JSON structure provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "# Define the feedback template as a regular string\n",
    "feedback_template = '''\n",
    "{\n",
    "    \"title\": \"Analysis of User Submission\",\n",
    "    \"analysis\": \"Detailed analysis of the user's approach.\",\n",
    "    \"summary\": \"Concise summary of the user's approach and correctness.\",\n",
    "    \"score\": \"An integer score: 1 (incorrect), 2 (partially correct), 3 (correct).\"\n",
    "}\n",
    "'''\n",
    "\n",
    "# Define the prompt\n",
    "prompt = f'''\n",
    "You are a professor who specializes in solving code problems.\n",
    "The user will submit, in words, their approach to solving the problem.\n",
    "Analyze the user's submission and provide feedback.\n",
    "\n",
    "Output your feedback in JSON format with the following structure:\n",
    "{feedback_template}\n",
    "\n",
    "Here are the problem details:\n",
    "\n",
    "Category: {problem_data['category']}\n",
    "Subcategory: {problem_data['subcategory']}\n",
    "Difficulty: {problem_data['difficulty']}\n",
    "Title: {problem_data['title']}\n",
    "Description: {problem_data['description']}\n",
    "Constraints: \n",
    "{chr(10).join([f'{constraint}' for constraint in problem_data['constraints']])}\n",
    "\n",
    "Examples:\n",
    "{chr(10).join([f'Input: {example[\"input\"]}, Output: {example[\"output\"]}, Explanation: {example[\"explanation\"]}' for example in problem_data['examples']])}\n",
    "\n",
    "Here is the user's submission:\n",
    "{user_submission}\n",
    "\n",
    "Ensure your response is coherent and follows the JSON structure provided.\n",
    "'''\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "# prompt = \"Can you summarize the capabilities of language models, including tasks such as summarization, question answering, and text generation?\"\n",
    "\n",
    "# Set pad_token to eos_token\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Tokenize the prompt and set attention mask\n",
    "# encoded_input = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Generate feedback with attention mask\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m      3\u001b[0m     input_ids\u001b[39m=\u001b[39mencoded_input[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m     attention_mask\u001b[39m=\u001b[39mencoded_input[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m250\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_new_tokens\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[1;32m      7\u001b[0m     do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,\n\u001b[1;32m      9\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[1;32m     10\u001b[0m     top_p\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m,\n\u001b[1;32m     11\u001b[0m     repetition_penalty \u001b[39m=\u001b[39m \u001b[39m5.0\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[39m# Decode the output\u001b[39;00m\n\u001b[1;32m     15\u001b[0m feedback_response \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output_ids[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate feedback with attention mask\n",
    "output_ids = model.generate(\n",
    "    input_ids=encoded_input['input_ids'],\n",
    "    attention_mask=encoded_input['attention_mask'],\n",
    "    max_length=250,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty = 5.0\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "feedback_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(feedback_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a professor who specializes in solving code problems.\\nThe user will submit, in words, their approach to solving the problem.\\nAnalyze the user\\'s submission and provide feedback.\\n\\nOutput your feedback in JSON format with the following structure:\\n\\n{\\n    \"title\": \"Analysis of User Submission\",\\n    \"analysis\": \"Detailed analysis of the user\\'s approach.\",\\n    \"summary\": \"Concise summary of the user\\'s approach and correctness.\",\\n    \"score\": \"An integer score: 1 (incorrect), 2 (partially correct), 3 (correct).\"\\n}\\n\\n\\nHere are the problem details:\\n\\nCategory: manipulations\\nSubcategory: strings\\nDifficulty: easy\\nTitle: Longest Palindrome\\nDescription: Given a string s which consists of lowercase or uppercase letters, return the length of the longest palindrome that can be built with those letters. Letters are case sensitive, for example, \\'Aa\\' is not considered a palindrome.\\nConstraints: \\n1 <= s.length <= 2000\\ns consists of lowercase and/or uppercase English letters only.\\n\\nExamples:\\nInput: abccccdd, Output: 7, Explanation: One longest palindrome that can be built is \\'dccaccd\\', whose length is 7.\\nInput: a, Output: 1, Explanation: The longest palindrome that can be built is \\'a\\', whose length is 1.\\n\\nHere is the user\\'s submission:\\n\\n    Create a count for each character in the string.\\n    For each character:\\n    Convert it to lowercase.\\n    Count how many times it appears.\\n    Set total_length to 0 and odd_count to 0.\\n    For each character count:\\n    Add the even part of the count to total_length.\\n    If the count is odd, add 1 to odd_count.\\n    If odd_count is greater than 0, add 1 to total_length.\\n    Return total_length.\\n\\n\\nEnsure your response is coherent and follows the JSON structure provided.\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> Output: 1']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n",
    "model_inputs = tokenizer(\n",
    "    [prompt], return_tensors=\"pt\", padding=True\n",
    ")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=500)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "LlamaForCausalLM is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/LlamaForCausalLM/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    403\u001b[0m         path_or_repo_id,\n\u001b[1;32m    404\u001b[0m         filename,\n\u001b[1;32m    405\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    406\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    407\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    408\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    409\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    410\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    411\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    412\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    413\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    414\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    415\u001b[0m     )\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[39mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1241\u001b[0m         \u001b[39m# Destination\u001b[39;49;00m\n\u001b[1;32m   1242\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1243\u001b[0m         \u001b[39m# File info\u001b[39;49;00m\n\u001b[1;32m   1244\u001b[0m         repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   1245\u001b[0m         filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m   1246\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   1247\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1248\u001b[0m         \u001b[39m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m         endpoint\u001b[39m=\u001b[39;49mendpoint,\n\u001b[1;32m   1250\u001b[0m         etag_timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1251\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1252\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1253\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1254\u001b[0m         \u001b[39m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1255\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1256\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1257\u001b[0m     )\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1347\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[39m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1349\u001b[0m \u001b[39m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[39m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1752\u001b[0m         url\u001b[39m=\u001b[39;49murl, proxies\u001b[39m=\u001b[39;49mproxies, timeout\u001b[39m=\u001b[39;49metag_timeout, headers\u001b[39m=\u001b[39;49mheaders, token\u001b[39m=\u001b[39;49mtoken\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1674\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1675\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1676\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1677\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1678\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1679\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1680\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1681\u001b[0m )\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    377\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    378\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    379\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    380\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    383\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:400\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 400\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    401\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-66d24e6b-6a277991123010c22cfd64c2;59e1fd92-c84c-4db7-9bc5-428727dccea8)\n\nRepository Not Found for url: https://huggingface.co/LlamaForCausalLM/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mLlamaForCausalLM\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m, pad_token_id\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mLlamaForCausalLM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:485\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m commit_hash \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    484\u001b[0m         \u001b[39m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m         resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    486\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m    487\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    488\u001b[0m             _raise_exceptions_for_gated_repo\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    489\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    490\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    491\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    492\u001b[0m         )\n\u001b[1;32m    493\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    494\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/development/codescript/venv/lib/python3.11/site-packages/transformers/utils/hub.py:425\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    421\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to have access to it at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 425\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    432\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    433\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: LlamaForCausalLM is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0,load_in_4bit=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to('cpu')\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,device='cpu')\n",
    "prompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:\"\n",
    "\n",
    "result = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\n",
    "model = model.to('cpu')\n",
    "# infer\n",
    "prompt = \"<human>: Who is Alan Turing?\\n<bot>:\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n",
    ")\n",
    "token = outputs.sequences[0, input_length:]\n",
    "output_str = tokenizer.decode(token)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_codescript",
   "language": "python",
   "name": "venv_codescript"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
