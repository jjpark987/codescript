{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample user submission\n",
    "message = '''\n",
    "    Create a count for each character in the string.\n",
    "    For each character:\n",
    "    Convert it to lowercase.\n",
    "    Count how many times it appears.\n",
    "    Set total_length to 0 and odd_count to 0.\n",
    "    For each character count:\n",
    "    Add the even part of the count to total_length.\n",
    "    If the count is odd, add 1 to odd_count.\n",
    "    If odd_count is greater than 0, add 1 to total_length.\n",
    "    Return total_length.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a professor who specializes in solving code problems.\n",
      "    The user will submit in words their approach to solving the problem.\n",
      "    Analyze the user's submission and provide feedback.\n",
      "\n",
      "    Output your feedback in JSON format with the following structure:\n",
      "    {\n",
      "        \"title\": \"Create a title for this analysis\",\n",
      "        \"analysis\": \"Create a detailed analysis on the user's submission\",\n",
      "        \"summary\": \"Create a concise one-paragraph summary of the analysis\",\n",
      "        \"score\": \"An integer score: 1 (incorrect), 2 (partially correct), 3 (correct)\"\n",
      "    }\n",
      "\n",
      "    Here are the problem details:\n",
      "    Category: \n",
      "    manipulations\n",
      "\n",
      "    Subcategory: \n",
      "    strings\n",
      "\n",
      "    Difficulty:\n",
      "    easy\n",
      "\n",
      "    Title: \n",
      "    Longest Palindrome\n",
      "\n",
      "    Description: \n",
      "    Given a string s which consists of lowercase or uppercase letters, return the length of the longest palindrome that can be built with those letters. Letters are case sensitive, for example, 'Aa' is not considered a palindrome.\n",
      "\n",
      "    Constraints: \n",
      "    1 <= s.length <= 2000\n",
      "s consists of lowercase and/or uppercase English letters only.\n",
      "\n",
      "    Examples:\n",
      "    Input: abccccdd, Output: 7, Explanation: One longest palindrome that can be built is 'dccaccd', whose length is 7.\n",
      "Input: a, Output: 1, Explanation: The longest palindrome that can be built is 'a', whose length is 1.\n",
      "\n",
      "    Here is the user's submission:\n",
      "    \n",
      "    Create a count for each character in the string.\n",
      "    For each character:\n",
      "    Convert it to lowercase.\n",
      "    Count how many times it appears.\n",
      "    Set total_length to 0 and odd_count to 0.\n",
      "    For each character count:\n",
      "    Add the even part of the count to total_length.\n",
      "    If the count is odd, add 1 to odd_count.\n",
      "    If odd_count is greater than 0, add 1 to total_length.\n",
      "    Return total_length.\n",
      "\n",
      "\n",
      "    Ensure your output is coherent and follows the JSON structure provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data\n",
    "import json\n",
    "\n",
    "with open('sample_problem.json', 'r') as file:\n",
    "    problem_data = json.load(file)\n",
    "\n",
    "# Define system_prompt\n",
    "system_prompt = f'''\n",
    "    You are a professor who specializes in solving code problems.\n",
    "    The user will submit in words their approach to solving the problem.\n",
    "    Analyze the user's submission and provide feedback.\n",
    "\n",
    "    Output your feedback in JSON format with the following structure:\n",
    "    {{\n",
    "        \"title\": \"Create a title for this analysis\",\n",
    "        \"analysis\": \"Create a detailed analysis on the user's submission\",\n",
    "        \"summary\": \"Create a concise one-paragraph summary of the analysis\",\n",
    "        \"score\": \"An integer score: 1 (incorrect), 2 (partially correct), 3 (correct)\"\n",
    "    }}\n",
    "\n",
    "    Here are the problem details:\n",
    "    Category: \n",
    "    {problem_data['category']}\n",
    "\n",
    "    Subcategory: \n",
    "    {problem_data['subcategory']}\n",
    "\n",
    "    Difficulty:\n",
    "    {problem_data['difficulty']}\n",
    "\n",
    "    Title: \n",
    "    {problem_data['title']}\n",
    "\n",
    "    Description: \n",
    "    {problem_data['description']}\n",
    "\n",
    "    Constraints: \n",
    "    {chr(10).join([f'{constraint}' for constraint in problem_data['constraints']])}\n",
    "\n",
    "    Examples:\n",
    "    {chr(10).join([f'Input: {example[\"input\"]}, Output: {example[\"output\"]}, Explanation: {example[\"explanation\"]}' for example in problem_data['examples']])}\n",
    "\n",
    "    Here is the user's submission:\n",
    "    {message}\n",
    "\n",
    "    Ensure your output is coherent and follows the JSON structure provided.\n",
    "'''\n",
    "\n",
    "print(system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/jjpark987/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c93b7f015f4118a3316032f458ecaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b917f86885046a995b0eee15eda5d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63278d83402f4783aa882ade8f9a045b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd0f84d1cfd4d29af8d895c8e0ce2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90574218c2f944a59b239ce903ce5826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d003658195a244aa87d654bb9361da34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff86eda7bbf44e1b11db6600ad3c454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c82ec5cad44870b81dabc6e611ce63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c0bbb68ad748dd84460789406ac2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model meta-llama/Meta-Llama-3.1-8B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4034, in from_pretrained\n    dispatch_model(model, **device_map_kwargs)\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/accelerate/big_modeling.py\", line 496, in dispatch_model\n    raise ValueError(\nValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4034, in from_pretrained\n    dispatch_model(model, **device_map_kwargs)\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/accelerate/big_modeling.py\", line 496, in dispatch_model\n    raise ValueError(\nValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m login(token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHUGGINGFACE_ACCESS_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a pirate chatbot who always responds in pirate speak!\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     22\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     26\u001b[0m     messages,\n\u001b[1;32m     27\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:299\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    298\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m         )\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model meta-llama/Meta-Llama-3.1-8B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4034, in from_pretrained\n    dispatch_model(model, **device_map_kwargs)\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/accelerate/big_modeling.py\", line 496, in dispatch_model\n    raise ValueError(\nValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 286, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 4034, in from_pretrained\n    dispatch_model(model, **device_map_kwargs)\n  File \"/Users/jjpark987/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/accelerate/big_modeling.py\", line 496, in dispatch_model\n    raise ValueError(\nValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\n\n\n"
     ]
    }
   ],
   "source": [
    "# Load Meta-Llama-3.1-8B\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv('HUGGINGFACE_ACCESS_TOKEN'), add_to_git_credential=True)\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA-2\n",
    "# from gradio_client import Client\n",
    "\n",
    "# client = Client('huggingface-projects/llama-2-13b-chat')\n",
    "# try:\n",
    "# \tresult = client.predict(\n",
    "# \t\tmessage=message,\n",
    "# \t\tsystem_prompt=system_prompt,\n",
    "# \t\tmax_new_tokens=1024,\n",
    "# \t\ttemperature=0.6,\n",
    "# \t\ttop_p=0.9,\n",
    "# \t\ttop_k=50,\n",
    "# \t\trepetition_penalty=1.2,\n",
    "# \t\tapi_name=\"/chat\"\n",
    "# \t)\n",
    "# except Exception as e:\n",
    "# \tprint(f'An error occurred: {e}')\n",
    "\n",
    "# print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
