{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample user submission\n",
    "message = '''\n",
    "    Create a count for each character in the string.\n",
    "    For each character:\n",
    "    Convert it to lowercase.\n",
    "    Count how many times it appears.\n",
    "    Set total_length to 0 and odd_count to 0.\n",
    "    For each character count:\n",
    "    Add the even part of the count to total_length.\n",
    "    If the count is odd, add 1 to odd_count.\n",
    "    If odd_count is greater than 0, add 1 to total_length.\n",
    "    Return total_length.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a professor who specializes in solving code problems.\n",
      "    The user will submit in words their approach to solving the problem.\n",
      "    Analyze the user's submission and provide feedback.\n",
      "\n",
      "    Output your feedback in JSON format with the following structure:\n",
      "    {\n",
      "        'title': 'Create a title for this analysis',\n",
      "        'analysis': 'Create a detailed analysis on the user's submission',\n",
      "        'summary': 'Create a concise one-paragraph summary of the analysis',\n",
      "        'score': 'An integer score: 1 (incorrect), 2 (partially correct), 3 (correct)'\n",
      "    }\n",
      "\n",
      "    Here are the problem details:\n",
      "    Category: \n",
      "    manipulations\n",
      "\n",
      "    Subcategory: \n",
      "    strings\n",
      "\n",
      "    Difficulty:\n",
      "    easy\n",
      "\n",
      "    Title: \n",
      "    Longest Palindrome\n",
      "\n",
      "    Description: \n",
      "    Given a string s which consists of lowercase or uppercase letters, return the length of the longest palindrome that can be built with those letters. Letters are case sensitive, for example, 'Aa' is not considered a palindrome.\n",
      "\n",
      "    Constraints: \n",
      "    1 <= s.length <= 2000\n",
      "s consists of lowercase and/or uppercase English letters only.\n",
      "\n",
      "    Examples:\n",
      "    Input: abccccdd, Output: 7, Explanation: One longest palindrome that can be built is 'dccaccd', whose length is 7.\n",
      "Input: a, Output: 1, Explanation: The longest palindrome that can be built is 'a', whose length is 1.\n",
      "\n",
      "    Here is the user's submission:\n",
      "    \n",
      "    Create a count for each character in the string.\n",
      "    For each character:\n",
      "    Convert it to lowercase.\n",
      "    Count how many times it appears.\n",
      "    Set total_length to 0 and odd_count to 0.\n",
      "    For each character count:\n",
      "    Add the even part of the count to total_length.\n",
      "    If the count is odd, add 1 to odd_count.\n",
      "    If odd_count is greater than 0, add 1 to total_length.\n",
      "    Return total_length.\n",
      "\n",
      "\n",
      "    Ensure your output is coherent and follows the JSON structure provided.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data\n",
    "import json\n",
    "\n",
    "with open('sample_problem.json', 'r') as file:\n",
    "    problem_data = json.load(file)\n",
    "\n",
    "# Define system_prompt\n",
    "system_prompt = f'''\n",
    "    You are a professor who specializes in solving code problems.\n",
    "    The user will submit in words their approach to solving the problem.\n",
    "    Analyze the user's submission and provide feedback.\n",
    "\n",
    "    Output your feedback in JSON format with the following structure:\n",
    "    {{\n",
    "        'title': 'Create a title for this analysis',\n",
    "        'analysis': 'Create a detailed analysis on the user's submission',\n",
    "        'summary': 'Create a concise one-paragraph summary of the analysis',\n",
    "        'score': 'An integer score: 1 (incorrect), 2 (partially correct), 3 (correct)'\n",
    "    }}\n",
    "\n",
    "    Here are the problem details:\n",
    "    Category: \n",
    "    {problem_data['category']}\n",
    "\n",
    "    Subcategory: \n",
    "    {problem_data['subcategory']}\n",
    "\n",
    "    Difficulty:\n",
    "    {problem_data['difficulty']}\n",
    "\n",
    "    Title: \n",
    "    {problem_data['title']}\n",
    "\n",
    "    Description: \n",
    "    {problem_data['description']}\n",
    "\n",
    "    Constraints: \n",
    "    {chr(10).join([f'{constraint}' for constraint in problem_data['constraints']])}\n",
    "\n",
    "    Examples:\n",
    "    {chr(10).join([f'Input: {example['input']}, Output: {example['output']}, Explanation: {example['explanation']}' for example in problem_data['examples']])}\n",
    "\n",
    "    Here is the user's submission:\n",
    "    {message}\n",
    "\n",
    "    Ensure your output is coherent and follows the JSON structure provided.\n",
    "'''\n",
    "\n",
    "print(system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/jjpark987/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680131cc96be43bcbf61b39740868d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     27\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Create the inference pipeline\u001b[39;00m\n\u001b[1;32m     32\u001b[0m text_generation_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Use 0 for MPS, -1 for CPU\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2905\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2902\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2904\u001b[0m         )\n\u001b[0;32m-> 2905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Desktop/codescript/codescript-backend/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "# Load Meta-Llama-3.1-8B\n",
    "from accelerate import disk_offload\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from torch import backends, float32\n",
    "\n",
    "# Load environment variables and Hugging Face token\n",
    "load_dotenv()\n",
    "login(token=os.getenv('HUGGINGFACE_ACCESS_TOKEN'), add_to_git_credential=True)\n",
    "\n",
    "model_id = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "\n",
    "# Load the model directly, which handles the weights\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=float32, \n",
    "    trust_remote_code=True, \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Apply disk offloading\n",
    "disk_offload(model=model, offload_dir='disk_offload')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "device = 'mps' if backends.mps.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Create the inference pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={'torch_dtype': float32},\n",
    "    # device_map='auto',\n",
    "    device=0 if device == 'mps' else -1  # Use 0 for MPS, -1 for CPU\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a pirate chatbot who always responds in pirate speak!'},\n",
    "    {'role': 'user', 'content': 'Who are you?'},\n",
    "]\n",
    "\n",
    "# Ensure model weights are loaded before inference\n",
    "outputs = text_generation_pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(outputs[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA-2\n",
    "# from gradio_client import Client\n",
    "\n",
    "# client = Client('huggingface-projects/llama-2-13b-chat')\n",
    "# try:\n",
    "# \tresult = client.predict(\n",
    "# \t\tmessage=message,\n",
    "# \t\tsystem_prompt=system_prompt,\n",
    "# \t\tmax_new_tokens=1024,\n",
    "# \t\ttemperature=0.6,\n",
    "# \t\ttop_p=0.9,\n",
    "# \t\ttop_k=50,\n",
    "# \t\trepetition_penalty=1.2,\n",
    "# \t\tapi_name='/chat'\n",
    "# \t)\n",
    "# except Exception as e:\n",
    "# \tprint(f'An error occurred: {e}')\n",
    "\n",
    "# print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
